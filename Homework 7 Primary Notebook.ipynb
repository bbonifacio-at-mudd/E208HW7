{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549eaa6d",
   "metadata": {},
   "source": [
    "# Team Viviane Solomon and Brandon Bonifacio\n",
    "# How We Split Up The Work: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b99cc3",
   "metadata": {},
   "source": [
    "# HW7: Train a Sequence Classifier That Can Predict if a Sentence is in English or Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a01c0",
   "metadata": {},
   "source": [
    "The goal of this assignment is to train a sequence classifier that can predict if a sentence is in \n",
    "English or Spanish. You should use the official PyTorch documentation to build your system \n",
    "from scratch. You may use other online sources as well but must cite your sources and indicate \n",
    "clearly what portions of your code have been copied and modified from elsewhere. You may \n",
    "work individually or with a partner on this assignment.\n",
    "\n",
    "\n",
    "Each team should submit one assignment as a single jupyter notebook on Sakai. At the top of \n",
    "your notebook, please indicate both team members’ names and who did what. To speed up \n",
    "training, you may want to run your jupyter notebook in Google Colab with a GPU. Note: The \n",
    "datasets provided below are very large, and you don’t need to train on everything!\n",
    "\n",
    "In fact, as you develop your code, I would recommend using a tiny subset of data to iterate quickly, and \n",
    "wait until your code is debugged to start training on larger subsets of data. It is much better to \n",
    "have a functioning model that is trained on 1% of the data than a non-functional model that \n",
    "failed to train on 100% of the data.\n",
    "\n",
    "An additional 10 points will be graded for the organization and clarity of your notebook. Your \n",
    "notebook should read like a tutorial and be understandable to others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381d41c",
   "metadata": {},
   "source": [
    "## Part 1: Basic System with fixed-length inputs (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f5793",
   "metadata": {},
   "source": [
    "In the first part of the assignment you will do the following:\n",
    "\n",
    "\n",
    "● Prepare the data (20 points). Get two large text files: one English file (WikiText-103, \n",
    "181MB) and one Spanish file (e.g. Spanish text corpus, 155MB). Convert to lowercase \n",
    "and remove all punctuation except “.” so the data only contains alphabet characters, \n",
    "whitespace, and periods. Determine a set of unique characters and map all characters \n",
    "to integers. Split the data into train & validation sets, and split each into chunks of fixed \n",
    "length.\n",
    "\n",
    "\n",
    "● Train 1-layer model (20 Points). Define an LSTM model containing 1 LSTM layer \n",
    "followed by an output linear layer. Your model should classify a fixed-length sequence \n",
    "of characters as English or Spanish. Show your training & validation loss curves, along \n",
    "with your validation classification accuracy.\n",
    "\n",
    "\n",
    "● Experimentation (20 points). Experiment with different aspects of the model: the \n",
    "number of LSTM layers, the number of fully connected layers, the size of the hidden \n",
    "layer, etc. Train the corresponding models, compare their performance, and provide \n",
    "plots to demonstrate the effect of at least two different hyperparameters of interest.\n",
    "\n",
    "\n",
    "● Intuition (5 points). Show the output of your model for several specific sentences. Pick \n",
    "inputs that demonstrate the behavior of the system, and try to figure out what things \n",
    "the model is focusing on. Explain your intuition about what the model is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b61763",
   "metadata": {},
   "source": [
    "## Welcome to our Tutorial for Preparing the Data! \n",
    "\n",
    "### In the cell below, we go through the process of converting the text to lowercase and removing all punctuation except \".\" so the data only contains alphabet characters, whitespace, and periods for the Spanish Sentences. We also save this locally so we don't have to do this every time we load the file.\n",
    "\n",
    "To provide an example for what we want to do with this data, we provide the first few sentences from the Spanish sentences.txt file. \n",
    "\n",
    "*la enciclopedia libre Jorge Hess De Wikipedia#\n",
    "\n",
    "*la enciclopedia libre Saltar a Jorge Hess de julio es un y cofundador de la Liga Argentina de Esperanto Hess escribió un manual para el aprendizaje de esperanto que fue editado por primera vez en y se titula Sabe Usted Esperanto#\n",
    "\n",
    "*Es uno de los más conocidos libros en español que tratan sobre el tema junto con Curso Práctico de Esperanto Ferenc Szilágyi#\n",
    "\n",
    "*el cual Hess adaptó para los en#\n",
    "\n",
    "\n",
    "## As you can see, each sentence begins with an aserisk (*), and it ends with a hashtag and a new-line character (#\\n) After this function, these sentences in the txt file should look like: \n",
    "\n",
    "la enciclopedia libre jorge hess de wikipedia\n",
    "la enciclopedia libre saltar a jorge hess de julio es un y cofundador de la liga argentina de esperanto hess escribió un manual para el aprendizaje de esperanto que fue editado por primera vez en y se titula sabe usted esperanto\n",
    "es uno de los más conocidos libros en español que tratan sobre el tema junto con curso práctico de esperanto ferenc szilágyi\n",
    "el cual hess adaptó para los en\n",
    "\n",
    "\n",
    "(note that there is a newline between each sentence, but that Jupyter Notebook combines lines that only differ by one \\n character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c25519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f034fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "áéíóúñü áéíóúñü.\n",
      "ÁÉÍÓÚÑÜ ÁÉÍÓÚÑÜ.\n"
     ]
    }
   ],
   "source": [
    "## There is an important aspect of Spanish sentences we must consider. Python is mostly an English-based language, \n",
    "## so it is possible that Python might miss the diacriticed characters, namely é, á, í, ó, ú, ñ, and ü. However, \n",
    "## thankfully the Python devs have already thought of this, so we don't have to worry about it. However, we will continually\n",
    "## check this throughout the process to make sure this is working as intended. \n",
    "## An example of Python functions working with Spanish characters is provided in the cell below. \n",
    "\n",
    "#Example of Python working with Spanish Characters - Python can work with Spanish!\n",
    "espanol = \"ÁÉÍÓÚÑÜ áéíóúñü.\"\n",
    "print(espanol.lower())  \n",
    "print(espanol.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9837a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi  my  name .... is brandón bonifacío. . . \n"
     ]
    }
   ],
   "source": [
    "#Here, we make the process helper function to process a single sentence according to the problem\n",
    "##Below, we convert the text in this file to lowercase, remove all punctuation except \".\"\n",
    "def process(sentence):\n",
    "    \"\"\"\n",
    "    Processes the given sentence string according to what the problem wants us to do:\n",
    "    - Convert all characters to lowercase.\n",
    "    - Remove all punctuation except \".\". Keep whitespace characters (\"\\n\" and \" \", idk any others)\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower() #convert all characters to lowercase, O(n) time \n",
    "    \n",
    "    #We are going to make a list of allowed characters! and if it's not in it, we get rid of it\n",
    "    #the instructions say to keep only alphabetic characters, periods, and whitespaces, and that is what we will \n",
    "    #consist the allowed characters with\n",
    "    \n",
    "    #we are going to use a dictionary for O(1) lookup time\n",
    "    allowed_characters = {\n",
    "        \" \": 0,\n",
    "        \"a\": 1,\n",
    "        \"b\": 2,\n",
    "        \"c\": 3, \n",
    "        \"d\": 4,\n",
    "        \"e\": 5, \n",
    "        \"f\": 6, \n",
    "        \"g\": 7, \n",
    "        \"h\": 8, \n",
    "        \"i\": 9, \n",
    "        \"j\": 10,\n",
    "        \"k\": 11, \n",
    "        \"l\": 12,\n",
    "        \"m\": 13, \n",
    "        \"n\": 14, \n",
    "        \"o\": 15,\n",
    "        \"p\": 16,\n",
    "        \"q\": 17, \n",
    "        \"r\": 18, \n",
    "        \"s\": 19, \n",
    "        \"t\": 20, \n",
    "        \"u\": 21, \n",
    "        \"v\": 22, \n",
    "        \"w\": 23, \n",
    "        \"x\": 24, \n",
    "        \"y\": 25, \n",
    "        \"z\": 26, \n",
    "        \".\": 27, \n",
    "        \"á\": 28,\n",
    "        \"é\": 29, \n",
    "        \"í\": 30,\n",
    "        \"ó\": 31, \n",
    "        \"ú\": 32,\n",
    "        \"ñ\": 33, \n",
    "        \"ü\": 34\n",
    "        }\n",
    "    \n",
    "    processed_sentence = ''.join(char for char in sentence if char in allowed_characters)\n",
    "    return processed_sentence\n",
    "\n",
    "test_string = \"Hi #%@ my 123415#$% name ..,3453,.,.343 !is B–RAND---ÓN B31onifacío. .@#$@$--=!~234324` . \"\n",
    "print(process(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448f084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████████████████████████████████████████| 6075660/6075660 [00:50<00:00, 120148.15it/s]\n",
      "Writing to file: 100%|███████████████████████████████████████████████████| 6075660/6075660 [00:09<00:00, 619557.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## sentences.txt is our file of Spanish sentences. \n",
    "## With respect to this Jupyter Notebook's directory, this raw file is stored in (for Brandon's computer): \n",
    "## /Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Raw\\Spanish/sentences.txt\n",
    "\n",
    "## After processing this data, we store it in \n",
    "## /E208/E208HW7/Data/Raw/Spanish/sentences.txt\n",
    "\n",
    "## AS AN IMPORTANT NOTE, THIS CELL SHOULD ONLY BE RUN ONCE. \n",
    "## UNCOMMENT THE CODE BELOW TO RUN IT:\n",
    "# Windows: Ctrl + / \n",
    "# Mac: Cmd + /\n",
    "\n",
    "\n",
    "# Path to the raw data\n",
    "input_path = \"/Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Raw\\Spanish/sentences.txt\"\n",
    "# Path to the processed data\n",
    "output_path = \"/Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Processed\\Spanish/sentences.txt\"\n",
    "\n",
    "#Open the input file and take out the sentences\n",
    "#https://stackoverflow.com/questions/2081836/how-to-read-specific-lines-from-a-file-by-line-number\n",
    "with open(input_path, \"r\") as file:\n",
    "    raw_sentences = file.readlines()\n",
    "\n",
    "#Process each sentence\n",
    "processed_sentences = []\n",
    "for sentence in tqdm(raw_sentences, desc=\"Processing sentences\"):\n",
    "    if sentence.startswith('*') and sentence.endswith(\"#\\n\"): #Every sentence had a * in front of it and the end character at the end\n",
    "        processed_sentences.append(process(sentence[1:-2]))\n",
    "    else:\n",
    "        print(f\"Something went wrong! Here's the current sentence: {sentence}\")\n",
    "        raise\n",
    "\n",
    "#Now write the processed sentences to the output file\n",
    "with open(output_path, \"w\") as file:\n",
    "    for sentence in tqdm(processed_sentences, desc=\"Writing to file\"):\n",
    "        if not sentence.endswith(\"\\n\"):\n",
    "            file.write(sentence + \"\\n\") #Add new line whitespace at the end of each sentence\n",
    "        else:\n",
    "            file.write(sentence)\n",
    "\n",
    "print(\"Data processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d40393",
   "metadata": {},
   "source": [
    "## Now that we have processed the Spanish sentences, we move on to processing the English sentences. However, the English sentences are in .tokens files, which we can open in VSCode. To provide an example of the text in the .tokens files, I provide the first few sentences from the file below: \n",
    "\n",
    "\n",
    " = Robert Boulter = \n",
    " \n",
    " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
    " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 2006 episode of the television series , Doctors , followed by a role in the 2007 theatre production of How to Curse directed by Josie Rourke . How to Curse was performed at Bush Theatre in the London Borough of Hammersmith and Fulham . Boulter starred in two films in 2008 , Daylight Robbery by filmmaker Paris <unk> , and Donkey Punch directed by Olly Blackburn . In May 2008 , Boulter made a guest appearance on a two @-@ part episode arc of the television series Waking the Dead , followed by an appearance on the television series Survivors in November 2008 . He had a recurring role in ten episodes of the television series Casualty in 2010 , as \" Kieron Fletcher \" . Boulter starred in the 2011 film Mercenaries directed by Paris <unk> . \n",
    " \n",
    " = = Career = = \n",
    " \n",
    " \n",
    " = = = 2000 – 2005 = = = \n",
    " \n",
    " In 2000 Boulter had a guest @-@ starring role on the television series The Bill ; he portrayed \" Scott Parry \" in the episode , \" In Safe Hands \" . Boulter starred as \" Scott \" in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . A review of Boulter 's performance in The Independent on Sunday described him as \" horribly menacing \" in the role , and he received critical reviews in The Herald , and Evening Standard . He appeared in the television series Judge John Deed in 2002 as \" <unk> Armitage \" in the episode \" Political <unk> \" , and had a role as a different character \" Toby Steele \" on The Bill . \n",
    " He had a recurring role in 2003 on two episodes of The Bill , as character \" Connor Price \" . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . Boulter starred as \" Darren \" , in the 2005 theatre productions of the Philip Ridley play Mercury Fur . It was performed at the Drum Theatre in Plymouth , and the <unk> Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . Boulter received a favorable review in The Daily Telegraph : \" The acting is shatteringly intense , with wired performances from Ben Whishaw ( now unrecognisable from his performance as Trevor Nunn 's Hamlet ) , Robert Boulter , Shane Zaza and Fraser Ayres . \" The Guardian noted , \" Ben Whishaw and Robert Boulter offer tenderness amid the savagery . \" \n",
    " \n",
    " = = = 2006 – present = = ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f24f4a",
   "metadata": {},
   "source": [
    "## As you can see, the formatting is a bit more complex than before. After processing, in order to format it in the same way as the spanish sentences, we want the post-processed sentences to look like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd4d32",
   "metadata": {},
   "source": [
    " robert boulter is an english film  television and theatre actor .\n",
    "he had a guest  starring role on the television series the bill in 2000 .\n",
    "this was followed by a starring role in the play herons written by simon stephens  which was performed in 2001 at the royal court theatre .\n",
    "he had a guest role in the television series judge john deed in 2002 .\n",
    "in 2004 boulter landed a role as  craig  in the episode  teddy s story  of the television series the long firm  he starred alongside actors mark strong and derek jacobi .\n",
    "he was cast in the 2005 theatre productions of the philip ridley play mercury fur  which was performed at the drum theatre in plymouth and the unk chocolate factory in london .\n",
    "he was directed by john tiffany and starred alongside ben whishaw  shane zaza  harry kent  fraser ayres  sophie stanton and dominic hall .\n",
    " in 2006  boulter starred alongside whishaw in the play citizenship written by mark ravenhill .\n",
    "he appeared on a 2006 episode of the television series  doctors  followed by a role in the 2007 theatre production of how to curse directed by josie rourke .\n",
    "how to curse was performed at bush theatre in the london borough of hammersmith and fulham .\n",
    "boulter starred in two films in 2008  daylight robbery by filmmaker paris unk  and donkey punch directed by olly blackburn .\n",
    "in may 2008  boulter made a guest appearance on a two  part episode arc of the television series waking the dead  followed by an appearance on the television series survivors in november 2008 .\n",
    "he had a recurring role in ten episodes of the television series casualty in 2010  as  kieron fletcher  .\n",
    "boulter starred in the 2011 film mercenaries directed by paris unk .\n",
    " in 2000 boulter had a guest  starring role on the television series the bill  he portrayed  scott parry  in the episode   in safe hands  .\n",
    "boulter starred as  scott  in the play herons written by simon stephens  which was performed in 2001 at the royal court theatre .\n",
    "a review of boulter s performance in the independent on sunday described him as  horribly menacing  in the role  and he received critical reviews in the herald  and evening standard .\n",
    "he appeared in the television series judge john deed in 2002 as  unk armitage  in the episode  political unk   and had a role as a different character  toby steele  on the bill .\n",
    " he had a recurring role in 2003 on two episodes of the bill  as character  connor price  .\n",
    "in 2004 boulter landed a role as  craig  in the episode  teddy s story  of the television series the long firm  he starred alongside actors mark strong and derek jacobi .\n",
    "boulter starred as  darren   in the 2005 theatre productions of the philip ridley play mercury fur .\n",
    "it was performed at the drum theatre in plymouth  and the unk chocolate factory in london .\n",
    "he was directed by john tiffany and starred alongside ben whishaw  shane zaza  harry kent  fraser ayres  sophie stanton and dominic hall .\n",
    "boulter received a favorable review in the daily telegraph   the acting is shatteringly intense  with wired performances from ben whishaw  now unrecognisable from his performance as trevor nunn s hamlet   robert boulter  shane zaza and fraser ayres .\n",
    " the guardian noted   ben whishaw and robert boulter offer tenderness amid the savagery ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf798f4",
   "metadata": {},
   "source": [
    "## To format this, we're going to follow the same approach as with the Spanish sentences, except we're going to take extra steps to get rid of inconsistent spacing or sentences that begin with a \"=\" because these aren't sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99fcc14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Going through sentences for wiki.test.tokens: 100%|████████████████████████████| 4358/4358 [00:00<00:00, 299509.69it/s]\n",
      "Going through sentences for wiki.train.tokens: 100%|█████████████████████| 1801350/1801350 [00:06<00:00, 295421.23it/s]\n",
      "Going through sentences for wiki.valid.tokens: 100%|███████████████████████████| 3760/3760 [00:00<00:00, 257386.46it/s]\n",
      "Processing sentences: 100%|███████████████████████████████████████████████| 4738808/4738808 [00:49<00:00, 95677.41it/s]\n",
      "Writing to file: 100%|██████████████████████████████████████████████████| 3950213/3950213 [00:03<00:00, 1037825.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##Note: This should only be run once, so uncomment this code when we need to make a new sentences.txt file\n",
    "\n",
    "# Path to the raw data\n",
    "input_path = \"/Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Raw\\English/\"\n",
    "# And the files we gotta process:\n",
    "files = [\"wiki.test.tokens\", \"wiki.train.tokens\", \"wiki.valid.tokens\"]\n",
    "# Path to the processed data\n",
    "output_path = \"/Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Processed\\English/sentences.txt\"\n",
    "\n",
    "#Open the input file and take out the sentences\n",
    "#https://stackoverflow.com/questions/2081836/how-to-read-specific-lines-from-a-file-by-line-number\n",
    "raw_sentences = []\n",
    "#Add the lines through each file to a single list\n",
    "for file in files:\n",
    "    #As a note, we have to use utf-8 here because it works better than regular open:\n",
    "    #https://stackoverflow.com/questions/36303919/what-encoding-does-open-use-by-default\n",
    "    with open(input_path +file, \"r\", encoding='utf-8') as reading_file:\n",
    "        sentences = reading_file.readlines() #initially split it by lines, this will allow us to skip \"=\" lines\n",
    "        for sentence in tqdm(sentences, desc=\"Going through sentences for \" + file): #go through each sentence\n",
    "            if len(sentence) > 3: #only keep sentences that aren't newlines and are actually sentences\n",
    "                if sentence[0] != \"=\" and sentence[0:2] != \" =\": #We don't want to keep the \"=\" lines\n",
    "                    real_sentences = sentence.split(\". \") #Once we have the sentences now, split by periods\n",
    "                    for real_sentence in real_sentences: #Go through each sentence we have now\n",
    "                        real_sentence = real_sentence.replace(\"<unk>\", \"\") #remove this annoying string that's EVERYWHERE in the data\n",
    "                        real_sentence = real_sentence.replace(\"  \", \" \") #replace double spaces\n",
    "                        real_sentence = real_sentence.replace(\" .\", \".\") #get rid of spaces before periods\n",
    "                        raw_sentences.append(real_sentence + \".\") #add the period back\n",
    "\n",
    "\n",
    "#Process each sentence\n",
    "processed_sentences = []\n",
    "for sentence in tqdm(raw_sentences, desc=\"Processing sentences\"):\n",
    "    if len(sentence) > 1: #We only want nonzero sentences\n",
    "        if sentence[0:2] != \" =\": #We don't want the sentences that start with an \" =\" as shown above\n",
    "            processed_sentence = process(sentence)\n",
    "            if len(processed_sentence) > 5: #5 is the shortest sentence possible in English with periods and spaces\n",
    "                #We needed to do the extra check to make sure its nonzero after processing because processing removes\n",
    "                #characters\n",
    "                processed_sentences.append(processed_sentence)\n",
    "\n",
    "#Now write the processed sentences to the output file\n",
    "with open(output_path, \"w\", encoding = 'utf-8') as file:\n",
    "    for sentence in tqdm(processed_sentences, desc=\"Writing to file\"):\n",
    "        file.write(sentence + \"\\n\") #Add new line whitespace at the end of each sentence, in same structure as espanol\n",
    "\n",
    "print(\"Data processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c14be4",
   "metadata": {},
   "source": [
    "## Now that we have processed the data as required by the problem, we now determine a set of unique characters and map all characters to integers. Because the problem stated that each sentence should only consist of alphabet characters, whitespace, and periods, we only include these in the unique character map. We also don't include the newline whitespace character because, as per the way we designed the sentences to be separated, the newline character is only used to separate the sentences in the txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73ee1b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_character_map = {\n",
    "    \" \": 0,\n",
    "    \"a\": 1,\n",
    "    \"b\": 2,\n",
    "    \"c\": 3, \n",
    "    \"d\": 4,\n",
    "    \"e\": 5, \n",
    "    \"f\": 6, \n",
    "    \"g\": 7, \n",
    "    \"h\": 8, \n",
    "    \"i\": 9, \n",
    "    \"j\": 10,\n",
    "    \"k\": 11, \n",
    "    \"l\": 12,\n",
    "    \"m\": 13, \n",
    "    \"n\": 14, \n",
    "    \"o\": 15,\n",
    "    \"p\": 16,\n",
    "    \"q\": 17, \n",
    "    \"r\": 18, \n",
    "    \"s\": 19, \n",
    "    \"t\": 20, \n",
    "    \"u\": 21, \n",
    "    \"v\": 22, \n",
    "    \"w\": 23, \n",
    "    \"x\": 24, \n",
    "    \"y\": 25, \n",
    "    \"z\": 26, \n",
    "    \".\": 27, \n",
    "    \"á\": 28,\n",
    "    \"é\": 29, \n",
    "    \"í\": 30,\n",
    "    \"ó\": 31, \n",
    "    \"ú\": 32,\n",
    "    \"ñ\": 33, \n",
    "    \"ü\": 34\n",
    "}\n",
    "\n",
    "english_file = \"/Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Processed\\English/sentences.txt\"\n",
    "\n",
    "spanish_file = \"/Users\\Brandon\\Desktop\\Classes\\E208\\Homework\\E208HW7\\Data\\Processed\\Spanish/sentences.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88a9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cced297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481e53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926b6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ed32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22edd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32886c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdab693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a56484b1",
   "metadata": {},
   "source": [
    "## We now split the data into train & validation sets, and split each into chunks of fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f4ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4459b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d654023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0af4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212a5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f9daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dac280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f46b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e48b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e108af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063443c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff38a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a3d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "356b0e1e",
   "metadata": {},
   "source": [
    "## Part 2: Realistic system with variable-length inputs (25 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb035d2",
   "metadata": {},
   "source": [
    "In the second part of the assignment you will do the following:\n",
    "\n",
    "\n",
    "● Prepare the data (10 points). Your data should be the same as in part 1, except that \n",
    "each sample should contain one complete sentence rather than a fixed-length sequence of characters. This means that your training & validation samples should have variable \n",
    "length. You will need to zero-pad your inputs.\n",
    "\n",
    "\n",
    "● Train model (15 points). Use the best model architecture that you found from part 1, \n",
    "and train a model on your data. Be careful to handle the zero-padding correctly, since \n",
    "you can no longer use the same index for all batch samples. Show the training & \n",
    "validation loss curves and validation accuracy. Compare your results to the \n",
    "corresponding model in part 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c19ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f32d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e0604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92dfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a03f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90fa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b545d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c34f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffda3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82122b22",
   "metadata": {},
   "source": [
    "Running List of Resources Used: \n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/20935151/how-to-encode-and-decode-from-spanish-in-python\n",
    "\n",
    "https://datagy.io/python-remove-punctuation-from-string/\"\n",
    "\n",
    "https://stackoverflow.com/questions/2081836/how-to-read-specific-lines-from-a-file-by-line-number\n",
    "\n",
    "https://docs.python.org/3/library/string.html\n",
    "\n",
    "https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/#download\n",
    "\n",
    "https://stackoverflow.com/questions/36303919/what-encoding-does-open-use-by-default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a3e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
