{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549eaa6d",
   "metadata": {},
   "source": [
    "# Team Viviane Solomon and Brandon Bonifacio\n",
    "# How We Split Up The Work: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b99cc3",
   "metadata": {},
   "source": [
    "# HW7: Train a Sequence Classifier That Can Predict if a Sentence is in English or Spanish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a01c0",
   "metadata": {},
   "source": [
    "The goal of this assignment is to train a sequence classifier that can predict if a sentence is in \n",
    "English or Spanish. You should use the official PyTorch documentation to build your system \n",
    "from scratch. You may use other online sources as well but must cite your sources and indicate \n",
    "clearly what portions of your code have been copied and modified from elsewhere. You may \n",
    "work individually or with a partner on this assignment.\n",
    "\n",
    "\n",
    "Each team should submit one assignment as a single jupyter notebook on Sakai. At the top of \n",
    "your notebook, please indicate both team members’ names and who did what. To speed up \n",
    "training, you may want to run your jupyter notebook in Google Colab with a GPU. Note: The \n",
    "datasets provided below are very large, and you don’t need to train on everything!\n",
    "\n",
    "In fact, as you develop your code, I would recommend using a tiny subset of data to iterate quickly, and \n",
    "wait until your code is debugged to start training on larger subsets of data. It is much better to \n",
    "have a functioning model that is trained on 1% of the data than a non-functional model that \n",
    "failed to train on 100% of the data.\n",
    "\n",
    "An additional 10 points will be graded for the organization and clarity of your notebook. Your \n",
    "notebook should read like a tutorial and be understandable to others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381d41c",
   "metadata": {},
   "source": [
    "## Part 1: Basic System with fixed-length inputs (65 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f5793",
   "metadata": {},
   "source": [
    "In the first part of the assignment you will do the following:\n",
    "\n",
    "\n",
    "● Prepare the data (20 points). Get two large text files: one English file (WikiText-103, \n",
    "181MB) and one Spanish file (e.g. Spanish text corpus, 155MB). Convert to lowercase \n",
    "and remove all punctuation except “.” so the data only contains alphabet characters, \n",
    "whitespace, and periods. Determine a set of unique characters and map all characters \n",
    "to integers. Split the data into train & validation sets, and split each into chunks of fixed \n",
    "length.\n",
    "\n",
    "\n",
    "● Train 1-layer model (20 Points). Define an LSTM model containing 1 LSTM layer \n",
    "followed by an output linear layer. Your model should classify a fixed-length sequence \n",
    "of characters as English or Spanish. Show your training & validation loss curves, along \n",
    "with your validation classification accuracy.\n",
    "\n",
    "\n",
    "● Experimentation (20 points). Experiment with different aspects of the model: the \n",
    "number of LSTM layers, the number of fully connected layers, the size of the hidden \n",
    "layer, etc. Train the corresponding models, compare their performance, and provide \n",
    "plots to demonstrate the effect of at least two different hyperparameters of interest.\n",
    "\n",
    "\n",
    "● Intuition (5 points). Show the output of your model for several specific sentences. Pick \n",
    "inputs that demonstrate the behavior of the system, and try to figure out what things \n",
    "the model is focusing on. Explain your intuition about what the model is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b61763",
   "metadata": {},
   "source": [
    "## Welcome to our Tutorial for Preparing the Data! \n",
    "\n",
    "### In the cell below, we go through the process of converting the text to lowercase and removing all punctuation except \".\" so the data only contains alphabet characters, whitespace, and periods for the Spanish Sentences. We also save this locally so we don't have to do this every time we load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448f084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentences.txt is our file of Spanish sentences. \n",
    "## With respect to this Jupyter Notebook's directory, this file is stored in: \n",
    "## /Data/Raw/Spanish/sequences.txt\n",
    "\n",
    "## After processing this data, we store it in \n",
    "## /Data/Raw/Spanish/processed_spanish.txt\n",
    "\n",
    "## The first few lines from this file are given below as an example:\n",
    "\n",
    "#*la enciclopedia libre Jorge Hess De Wikipedia#\n",
    "#*la enciclopedia libre Saltar a Jorge Hess de julio es un y cofundador de la Liga Argentina de Esperanto Hess escribió un manual para el aprendizaje de esperanto que fue editado por primera vez en y se titula Sabe Usted Esperanto#\n",
    "#*Es uno de los más conocidos libros en español que tratan sobre el tema junto con Curso Práctico de Esperanto Ferenc Szilágyi#\n",
    "#*el cual Hess adaptó para los en#\n",
    "\n",
    "## As you can see, each sentence begins with an aserisk (*), and it ends with a hashtag and a new-line character (#\\n)\n",
    "\n",
    "\n",
    "##Below, we convert the text in this file to lowercase, remove all punctuation except \".\", by ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f4ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4459b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e108af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063443c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff38a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a3d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "356b0e1e",
   "metadata": {},
   "source": [
    "## Part 2: Realistic system with variable-length inputs (25 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb035d2",
   "metadata": {},
   "source": [
    "In the second part of the assignment you will do the following:\n",
    "\n",
    "\n",
    "● Prepare the data (10 points). Your data should be the same as in part 1, except that \n",
    "each sample should contain one complete sentence rather than a fixed-length sequence of characters. This means that your training & validation samples should have variable \n",
    "length. You will need to zero-pad your inputs.\n",
    "\n",
    "\n",
    "● Train model (15 points). Use the best model architecture that you found from part 1, \n",
    "and train a model on your data. Be careful to handle the zero-padding correctly, since \n",
    "you can no longer use the same index for all batch samples. Show the training & \n",
    "validation loss curves and validation accuracy. Compare your results to the \n",
    "corresponding model in part 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c19ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f32d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e0604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e92dfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a03f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90fa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b545d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c34f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffda3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82122b22",
   "metadata": {},
   "source": [
    "Running List of Resources Used: \n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/20935151/how-to-encode-and-decode-from-spanish-in-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a3e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
